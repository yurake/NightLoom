#!/usr/bin/env python3
"""
è»¸IDä¸æ•´åˆä¿®æ­£ã®çµ±åˆãƒ†ã‚¹ãƒˆ

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ä»¥ä¸‹ã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã™ï¼š
1. æ–°ã—ã„é…åˆ—å½¢å¼ã®é‡ã¿æ§‹é€ 
2. ä¸‹ä½äº’æ›æ€§ï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼dictå½¢å¼ï¼‰
3. OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
4. Pydanticãƒ¢ãƒ‡ãƒ«ã®å‹•ä½œ
"""

import asyncio
from uuid import uuid4
from app.models.session import Session, Axis, Choice, WeightEntry
from app.clients.openai_client import OpenAIClient, ScenarioResponse
from app.models.llm_config import ProviderConfig, LLMProvider


def test_legacy_dict_format():
    """ãƒ¬ã‚¬ã‚·ãƒ¼dictå½¢å¼ã®ãƒ†ã‚¹ãƒˆ"""
    print("ðŸ§ª Testing legacy dict format...")
    
    session = Session(
        id=uuid4(),
        initialCharacter='ã‚',
        themeId='test',
        axes=[
            Axis(id='axis_1', name='Logic vs Emotion', description='ãƒ†ã‚¹ãƒˆè»¸1', direction='è«–ç†âŸ·æ„Ÿæƒ…'),
            Axis(id='axis_2', name='Speed vs Caution', description='ãƒ†ã‚¹ãƒˆè»¸2', direction='è¿…é€ŸâŸ·æ…Žé‡')
        ]
    )
    
    legacy_choice = Choice(
        id='choice_1',
        text='ãƒ¬ã‚¬ã‚·ãƒ¼é¸æŠžè‚¢',
        weights={'axis_1': 0.8, 'axis_2': -0.3}
    )
    
    # å¤‰æ›ãƒ¡ã‚½ãƒƒãƒ‰ã®ãƒ†ã‚¹ãƒˆ
    weights_dict = legacy_choice.get_weights_dict()
    weights_array = legacy_choice.get_weights_array()
    
    assert weights_dict == {'axis_1': 0.8, 'axis_2': -0.3}
    assert len(weights_array) == 2
    assert weights_array[0].id == 'axis_1'
    assert weights_array[0].score == 0.8
    
    print("âœ… Legacy dict format test passed!")


def test_new_array_format():
    """æ–°ã—ã„é…åˆ—å½¢å¼ã®ãƒ†ã‚¹ãƒˆ"""
    print("ðŸ§ª Testing new array format...")
    
    new_choice = Choice(
        id='choice_2',
        text='æ–°ã—ã„é¸æŠžè‚¢',
        weights=[
            WeightEntry(id='axis_1', name='Logic vs Emotion', score=0.5),
            WeightEntry(id='axis_2', name='Speed vs Caution', score=-0.7),
            WeightEntry(id='axis_3', name='Independence vs Collaboration', score=0.2),
            WeightEntry(id='axis_4', name='Risk Taking vs Safety', score=-0.1)
        ]
    )
    
    # å¤‰æ›ãƒ¡ã‚½ãƒƒãƒ‰ã®ãƒ†ã‚¹ãƒˆ
    weights_dict = new_choice.get_weights_dict()
    weights_array = new_choice.get_weights_array()
    
    expected_dict = {
        'axis_1': 0.5, 'axis_2': -0.7, 'axis_3': 0.2, 'axis_4': -0.1
    }
    assert weights_dict == expected_dict
    assert len(weights_array) == 4
    assert all(isinstance(w, WeightEntry) for w in weights_array)
    
    print("âœ… New array format test passed!")


async def test_openai_validation():
    """OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ"""
    print("ðŸ§ª Testing OpenAI client validation...")
    
    config = ProviderConfig(
        provider=LLMProvider.OPENAI,
        api_key='test-key',
        model_name='gpt-4'
    )
    client = OpenAIClient(config)
    
    # å•é¡Œã®ã‚ã£ãŸå½¢å¼ï¼ˆè»¸åãŒä¸æ­£ï¼‰
    problematic_response = {
        'scene': {
            'scene_index': 1,
            'narrative': 'ã‚ãªãŸã¯é‡è¦ãªãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªãƒ¼ãƒ€ãƒ¼ã¨ã—ã¦ã€ãƒãƒ¼ãƒ ã®æ–¹å‘æ€§ã‚’æ±ºã‚ã‚‹é‡è¦ãªæ±ºæ–­ã‚’è¿«ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚é™ã‚‰ã‚ŒãŸæ™‚é–“ã®ä¸­ã§ã€è¤‡æ•°ã®é¸æŠžè‚¢ã‹ã‚‰æœ€é©ãªåˆ¤æ–­ã‚’ä¸‹ã™å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã“ã®æ±ºå®šãŒãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã®æˆåŠŸã«å¤§ããå½±éŸ¿ã™ã‚‹ã“ã¨ã¯æ˜Žã‚‰ã‹ã§ã™ã€‚',
            'choices': [
                {
                    'id': 'choice_1_1',
                    'text': 'æ…Žé‡ã«ãƒ‡ãƒ¼ã‚¿åˆ†æžã‚’ã—ã¦æ±ºã‚ã‚‹',
                    'weights': {
                        'Ambition': 0.0,  # æ­£ã—ããªã„è»¸å
                        'Growth Mindset': 0.5,
                        'Self-Reflection': 0.3,
                        'Resilience': 0.0
                    }
                },
                {
                    'id': 'choice_1_2',
                    'text': 'ç›´æ„Ÿã§æ±ºã‚ã‚‹',
                    'weights': {
                        'Ambition': 0.8,
                        'Growth Mindset': 0.0,
                        'Self-Reflection': -0.5,
                        'Resilience': 0.3
                    }
                },
                {
                    'id': 'choice_1_3',
                    'text': 'ãƒãƒ¼ãƒ ã¨ç›¸è«‡',
                    'weights': {
                        'Ambition': -0.3,
                        'Growth Mindset': 0.2,
                        'Self-Reflection': 0.8,
                        'Resilience': 0.0
                    }
                },
                {
                    'id': 'choice_1_4',
                    'text': 'ãƒªã‚¹ã‚¯ã‚’é¿ã‘ã‚‹',
                    'weights': {
                        'Ambition': -0.8,
                        'Growth Mindset': -0.3,
                        'Self-Reflection': 0.0,
                        'Resilience': -0.5
                    }
                }
            ]
        }
    }
    
    # æ­£ã—ã„æ–°ã—ã„å½¢å¼
    fixed_response = {
        'scene': {
            'scene_index': 1,
            'narrative': 'ã‚ãªãŸã¯é‡è¦ãªãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªãƒ¼ãƒ€ãƒ¼ã¨ã—ã¦ã€ãƒãƒ¼ãƒ ã®æ–¹å‘æ€§ã‚’æ±ºã‚ã‚‹é‡è¦ãªæ±ºæ–­ã‚’è¿«ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚é™ã‚‰ã‚ŒãŸæ™‚é–“ã®ä¸­ã§ã€è¤‡æ•°ã®é¸æŠžè‚¢ã‹ã‚‰æœ€é©ãªåˆ¤æ–­ã‚’ä¸‹ã™å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã“ã®æ±ºå®šãŒãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã®æˆåŠŸã«å¤§ããå½±éŸ¿ã™ã‚‹ã“ã¨ã¯æ˜Žã‚‰ã‹ã§ã™ã€‚',
            'choices': [
                {
                    'id': 'choice_1_1',
                    'text': 'æ…Žé‡ã«ãƒ‡ãƒ¼ã‚¿åˆ†æžã‚’ã—ã¦æ±ºã‚ã‚‹',
                    'weights': [
                        {'id': 'axis_1', 'name': 'Logic vs Emotion', 'score': 0.8},
                        {'id': 'axis_2', 'name': 'Speed vs Caution', 'score': -0.6},
                        {'id': 'axis_3', 'name': 'Independence vs Collaboration', 'score': 0.3},
                        {'id': 'axis_4', 'name': 'Risk Taking vs Safety', 'score': -0.4}
                    ]
                },
                {
                    'id': 'choice_1_2',
                    'text': 'ç›´æ„Ÿã§æ±ºã‚ã‚‹',
                    'weights': [
                        {'id': 'axis_1', 'name': 'Logic vs Emotion', 'score': -0.7},
                        {'id': 'axis_2', 'name': 'Speed vs Caution', 'score': 1.0},
                        {'id': 'axis_3', 'name': 'Independence vs Collaboration', 'score': 0.8},
                        {'id': 'axis_4', 'name': 'Risk Taking vs Safety', 'score': 0.6}
                    ]
                },
                {
                    'id': 'choice_1_3',
                    'text': 'ãƒãƒ¼ãƒ ã¨ç›¸è«‡',
                    'weights': [
                        {'id': 'axis_1', 'name': 'Logic vs Emotion', 'score': 0.2},
                        {'id': 'axis_2', 'name': 'Speed vs Caution', 'score': -0.5},
                        {'id': 'axis_3', 'name': 'Independence vs Collaboration', 'score': -1.0},
                        {'id': 'axis_4', 'name': 'Risk Taking vs Safety', 'score': 0.0}
                    ]
                },
                {
                    'id': 'choice_1_4',
                    'text': 'ãƒªã‚¹ã‚¯ã‚’é¿ã‘ã‚‹',
                    'weights': [
                        {'id': 'axis_1', 'name': 'Logic vs Emotion', 'score': 0.4},
                        {'id': 'axis_2', 'name': 'Speed vs Caution', 'score': -0.8},
                        {'id': 'axis_3', 'name': 'Independence vs Collaboration', 'score': -0.2},
                        {'id': 'axis_4', 'name': 'Risk Taking vs Safety', 'score': -1.0}
                    ]
                }
            ]
        }
    }
    
    template_data = {
        'session_id': 'test-session',
        'scene_index': 1,
        'keyword': 'æˆé•·',
        'axes': [
            {'id': 'axis_1', 'name': 'Logic vs Emotion'},
            {'id': 'axis_2', 'name': 'Speed vs Caution'},
            {'id': 'axis_3', 'name': 'Independence vs Collaboration'},
            {'id': 'axis_4', 'name': 'Risk Taking vs Safety'}
        ]
    }
    
    # å•é¡Œã®ã‚ã‚‹å½¢å¼ã¯è­¦å‘Šã‚’å‡ºã™ãŒé€šã™ã‚ˆã†ã«ãªã£ã¦ã„ã‚‹
    try:
        await client._validate_scenario_response(problematic_response, template_data)
        print("âš ï¸  Problematic format passed with warnings (as expected)")
    except Exception as e:
        print(f"âŒ Problematic format failed: {e}")
    
    # æ­£ã—ã„å½¢å¼ã¯é€šã‚‹
    try:
        await client._validate_scenario_response(fixed_response, template_data)
        print("âœ… Fixed format validation passed!")
    except Exception as e:
        print(f"âŒ Fixed format failed: {e}")
        raise
    
    # Pydanticãƒ¢ãƒ‡ãƒ«ã®æ¤œè¨¼
    try:
        scenario_model = ScenarioResponse.model_validate(fixed_response)
        print("âœ… Pydantic model validation passed!")
        
        # é‡ã¿æƒ…å ±ã®ç¢ºèª
        choice = scenario_model.scene.choices[0]
        print(f"   First choice weights: {[f'{w.id}={w.score}' for w in choice.weights]}")
        
    except Exception as e:
        print(f"âŒ Pydantic validation failed: {e}")
        raise


def test_weight_conversion():
    """é‡ã¿å¤‰æ›ã®ãƒ†ã‚¹ãƒˆ"""
    print("ðŸ§ª Testing weight conversion utilities...")
    
    # dict -> array conversion test
    dict_weights = {'axis_1': 0.5, 'axis_2': -0.3, 'axis_3': 0.8, 'axis_4': 0.0}
    axis_mapping = {
        'axis_1': 'Logic vs Emotion',
        'axis_2': 'Speed vs Caution',
        'axis_3': 'Independence vs Collaboration',
        'axis_4': 'Risk Taking vs Safety'
    }
    
    array_weights = [
        {'id': axis_id, 'name': axis_mapping[axis_id], 'score': score}
        for axis_id, score in dict_weights.items()
        if axis_id in axis_mapping
    ]
    
    assert len(array_weights) == 4
    assert array_weights[0]['id'] == 'axis_1'
    assert array_weights[0]['name'] == 'Logic vs Emotion'
    assert array_weights[0]['score'] == 0.5
    
    # array -> dict conversion test
    converted_dict = {w['id']: w['score'] for w in array_weights}
    assert converted_dict == dict_weights
    
    print("âœ… Weight conversion test passed!")


async def main():
    """ãƒ¡ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    print("ðŸš€ Starting axis ID mismatch fix integration tests...\n")
    
    try:
        test_legacy_dict_format()
        print()
        
        test_new_array_format()
        print()
        
        await test_openai_validation()
        print()
        
        test_weight_conversion()
        print()
        
        print("ðŸŽ‰ All tests passed! Axis ID mismatch fix is working correctly.")
        
    except Exception as e:
        print(f"ðŸ’¥ Test failed: {e}")
        raise


if __name__ == '__main__':
    asyncio.run(main())
